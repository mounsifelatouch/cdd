{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mounsifelatouch/cdd/blob/master/notebooks/4_cdd_ml_part_4_model_building.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AAfh_j7hSwQ"
      },
      "source": [
        "# **Bioinformatics Project - Computational Drug Discovery [Part 4] Classification Models**\n",
        "\n",
        "**MOUNSIF EL ATOUCH**\n",
        "\n",
        "In this Jupyter notebook, we will be building a machine learning model using the ChEMBL bioactivity data.\n",
        "\n",
        "In **Part 4**, we will be building classification models\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsPbzowRSvzc"
      },
      "source": [
        "## **1. Installing librairies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw7MqTMphSwR"
      },
      "source": [
        "## **2. Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3rFTNAIhSwS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.spatial.distance import *\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "seed = 42\n",
        "\n",
        "from sklearn.model_selection import *\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.metrics import *\n",
        "\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "from scipy import interp\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWF2oygoUIEX"
      },
      "outputs": [],
      "source": [
        "def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n",
        "    if train:\n",
        "        pred = clf.predict(X_train)\n",
        "        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n",
        "        print(\"Train Result:\\n================================================\")\n",
        "        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n",
        "        \n",
        "    elif train==False:\n",
        "        pred = clf.predict(X_test)\n",
        "        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n",
        "        print(\"Test Result:\\n================================================\")        \n",
        "        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ts7qg-SnfZ4"
      },
      "outputs": [],
      "source": [
        "def print_auc(clf, X_train, y_train, X_test, y_test, model_name) :\n",
        "    \n",
        "    clf = clf.fit(X_train, y_train)\n",
        "    y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
        "    fpr, tpr, thr = roc_curve(y_test,  y_pred_proba)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    plt.plot(fpr, tpr, label=model_name+\" auc=\"+str(roc_auc))\n",
        "    plt.legend(loc=4)\n",
        "    plt.title(\"ROC curve\")\n",
        "\n",
        "    plt.show()\n",
        "    # Data to plot precision - recall curve\n",
        "    precision, recall, thresholds_log = precision_recall_curve(y_test, y_pred_proba)\n",
        "    # Use AUC function to calculate the area under the curve of precision recall curve\n",
        "    auc_precision_recall = auc(recall, precision)\n",
        "    \n",
        "    plt.plot(recall, precision, label=model_name+\" auc_prc=\"+str(auc_precision_recall))\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.legend(loc=4)\n",
        "    plt.title(\"Precision-Recall Curve\")\n",
        "\n",
        "    plt.show()\n",
        "    print_score(clf, X_train, y_train, X_test, y_test, train=True)\n",
        "    print_score(clf, X_train, y_train, X_test, y_test, train=False)\n",
        "    precision_sc=precision_score(y_test, clf.predict(X_test), average=None)\n",
        "    recall_sc = recall_score(y_test, clf.predict(X_test), average=None)\n",
        "    test_score = accuracy_score(y_test, clf.predict(X_test)) * 100\n",
        "    train_score = accuracy_score(y_train, clf.predict(X_train)) * 100\n",
        "    f1 = f1_score(y_test, clf.predict(X_test))\n",
        "    print_score(clf, X_train, y_train, X_test, y_test, train=True)\n",
        "    metriques = {\"fpr\" : fpr, \"recall_sc\" : recall_sc, \"precision_sc\" : precision_sc, \"tpr\" : tpr,\n",
        "                 \"test_score\" : test_score, \"train_score\" : train_score, \"f1\" : f1,\n",
        "                 \"auc_pr\" : auc_precision_recall, \"auc\": roc_auc}\n",
        "    return metriques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E4phmZn-u_n"
      },
      "outputs": [],
      "source": [
        "def metrics(X_train, X_test, y_train, y_test, model):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    print(\"Training set accuracy: {:.2f}\".format(model.score(X_train, y_train)))\n",
        "    print(\"Test set accuracy: {:.2f}\".format(model.score(X_test, y_test)))\n",
        "    print(\"Precision: {:.2f}\".format(precision_score(y_test, y_pred)))\n",
        "    print(\"Recall: {:.2f}\".format(recall_score(y_test, y_pred)))\n",
        "    print(\"F1 score: {:.2f}\".format(f1_score(y_test, y_pred)))\n",
        "    print(\"ROC AUC score: {:.2f}\".format(roc_auc_score(y_test, y_proba)))\n",
        "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "    print(\"True Negatives: {:.0f}\".format(tn))\n",
        "    print(\"False Positives: {:.0f}\".format(fp))\n",
        "    print(\"False Negatives: {:.0f}\".format(fn))\n",
        "    print(\"TruePositives: {:.0f}\".format(tp))\n",
        "    print(\"-------------------------------------\")\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GvT3PArhSwX"
      },
      "source": [
        "## **4. Load the data set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stvhigCCaSgS"
      },
      "outputs": [],
      "source": [
        "! wget https://raw.githubusercontent.com/mounsifelatouch/cdd/master/data/bioactivity_data_PubchemFingerprinter.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSgppzqPiR0G"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('bioactivity_data_PubchemFingerprinter.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuiiC0xthSwb"
      },
      "source": [
        "## **5. Input features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCeQQn0uhSwb"
      },
      "source": [
        "### **5.1. Input features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li32nAPohSwc",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "X = df.drop('Activity', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGQjCQtfhSwg"
      },
      "source": [
        "### **5.2. Output features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWylAtAVhSwh"
      },
      "outputs": [],
      "source": [
        "y = df['Activity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nRIG2ts9gnf"
      },
      "outputs": [],
      "source": [
        "# count the number of instances in each class\n",
        "counts = y.value_counts()\n",
        "\n",
        "# calculate the ratio of the negative class to the positive class\n",
        "imbalance_ratio = counts[0] / counts[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-zGSqXohSwx"
      },
      "source": [
        "### **5.3. Let's examine the data dimension**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhT04XtLhSwx"
      },
      "outputs": [],
      "source": [
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtaFgHI9aSgX"
      },
      "source": [
        "### **5.4. Remove low variance features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHh6_KoBaSgX"
      },
      "outputs": [],
      "source": [
        "# Create a VarianceThreshold object\n",
        "selector = VarianceThreshold(threshold=.01)\n",
        "\n",
        "# Fit the selector to the data and transform the data\n",
        "selector.fit_transform(X)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_features = X.columns[selected_indices]\n",
        "\n",
        "X_reduced = X[selected_features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A_sKkiiaSgY"
      },
      "outputs": [],
      "source": [
        "X.shape, X_reduced.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjhOlkOVhSxR"
      },
      "source": [
        "## **6. Data split (80/20 ratio)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be-1xtR_aSgZ"
      },
      "outputs": [],
      "source": [
        "n = np.arange(len(X_reduced))\n",
        "idx_train, idx_test = train_test_split(n, stratify=y, test_size=.2, random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yO0-rdxraSga"
      },
      "outputs": [],
      "source": [
        "X1_train, y1_train = X_reduced.loc[idx_train], y.loc[idx_train]\n",
        "X1_test, y1_test = X_reduced.loc[idx_test], y.loc[idx_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mTBEdJzaSga"
      },
      "outputs": [],
      "source": [
        "X1_train.shape, y1_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJnMW1a-aSgb"
      },
      "outputs": [],
      "source": [
        "X1_test.shape, y1_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKDG3FL2aSgb"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuAqsIWUaSgb"
      },
      "outputs": [],
      "source": [
        "X = X1_train.reset_index(drop=True)\n",
        "y = y1_train.reset_index(drop=True)\n",
        "X1_test = X1_test.reset_index(drop=True)\n",
        "y1_test = y1_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq6n06GjaSgc"
      },
      "outputs": [],
      "source": [
        "n_samples, n_classes = X.shape[0], 2\n",
        "class_weights = dict(zip(np.unique(y), n_samples / (n_classes * np.bincount(y))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oC9LSpsD9HN"
      },
      "source": [
        "## **7. Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNMPvtib-8aN"
      },
      "source": [
        "## **Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvl__jcwDJr5"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf1 = LogisticRegression()\n",
        "\n",
        "cross_val = StratifiedKFold(n_splits=5)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5.5, 5.5))\n",
        "\n",
        "tprs, aucs, y_real, y_prob, prs = [], [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "    \n",
        "plt.figure(figsize=(5.5 , 5.5))\n",
        "\n",
        "accs, precs, recs, f1_scr = [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i=0\n",
        "\n",
        "space = dict()\n",
        "space['class_weight'] = [class_weights, None, 'balanced']\n",
        "space['solver'] = ['lbfgs', 'liblinear', 'newton-cg', 'sag']\n",
        "space['penalty'] = ['l1', 'l2', 'none']\n",
        "space['C'] = uniform(0.1, 100)\n",
        "space['max_iter'] = randint(100, 500)\n",
        "\n",
        "\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "lst_accu_stratified = []\n",
        "\n",
        "index_iterator = cross_val.split(X, y)\n",
        "\n",
        "clf_random = RandomizedSearchCV(estimator=clf1, param_distributions=space, n_iter=100, scoring='f1', cv=cross_val, verbose=2, n_jobs=-1)\n",
        "for train_index, test_index in index_iterator:\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "  \n",
        "    clf_random.fit(X_train, y_train)\n",
        "    pred_proba = clf_random.predict_proba(X_test)\n",
        "    \n",
        "    precision, recall, _ = precision_recall_curve(y_test, pred_proba[:, 1])\n",
        "    prs.append(interp(mean_recall, precision, recall))\n",
        "    pr_auc = auc(recall, precision)\n",
        "    aucs.append(pr_auc)\n",
        "    plt.plot(recall, precision, lw=3, alpha=0.5, label='Fold %d (AUCPR = %0.2f)' % (i+1, pr_auc))\n",
        "\n",
        "    viz = RocCurveDisplay.from_estimator(clf_random, X_test, y_test, name='ROC fold {}'.format(i+1), ax=ax, alpha=.3)\n",
        "\n",
        "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(viz.roc_auc)\n",
        "\n",
        "    lst_accu_stratified.append(clf_random.score(X_test, y_test))\n",
        "    y_train_pred = clf_random.predict(X_train)\n",
        "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel())*100\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    y_test_pred = clf_random.predict(X_test)\n",
        "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel())*100\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('Best Score: %s'.format(i) % clf_random.best_score_)\n",
        "    print('Best Hyperparameters: %s'.format(i) % clf_random.best_params_)\n",
        "    acc = accuracy_score(y_test, y_test_pred)\n",
        "    prec = precision_score(y_test, y_test_pred)\n",
        "    rec = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test,y_test_pred)\n",
        "    accs.append(acc)\n",
        "    precs.append(prec)\n",
        "    recs.append(rec)\n",
        "    f1_scr.append(f1)\n",
        "    print(f'Accuracy: {acc}, Precision: {prec}, Recall: {rec}, f1_score: {f1}')\n",
        "    i=i+1\n",
        "\n",
        "\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', lw=.7, color='black', label='Chance', alpha=.7)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "ax.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=.7, alpha=.7)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.7, label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic for LGBM\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot([0, 1], [1, 0], linestyle='--', lw=.7, color='k', label='Luck', alpha=.7)\n",
        "mean_precision = np.mean(prs, axis=0)\n",
        "mean_auc = auc(mean_recall, mean_precision)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_precision, mean_recall, color='navy',label=r'Mean (AUCPR = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=4)\n",
        "    \n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('Recall' ,  fontweight = \"bold\" , fontsize=10)\n",
        "plt.ylabel('Precision',fontweight = \"bold\" , fontsize=10)\n",
        "plt.tick_params(axis='both', which='major', labelsize=10)\n",
        "plt.legend( prop={'size':10} , loc = 0)\n",
        "plt.show()\n",
        "\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "print('List of possible accuracy: \\n', lst_accu_stratified)\n",
        "print('Maximum Accuracy That can be obtained from this model is: \\n', max(lst_accu_stratified)*100, '%')\n",
        "print('Minimum Accuracy: \\n', min(lst_accu_stratified)*100, '%')\n",
        "print('Overall Accuracy: \\n', mean(lst_accu_stratified)*100, '%')\n",
        "print('Standard Deviation is: \\n', stdev(lst_accu_stratified))\n",
        "\n",
        "print('List of possible accuracy for trainning:', train_accuracy_list)\n",
        "print('List of possible accuracy for testing:', test_accuracy_list)\n",
        "\n",
        "print(f'Mean Accuracy: {np.mean(accs)}, Mean Precision: {np.mean(precs)}, Mean Recall: {np.mean(recs)}, Mean f1_score: {np.mean(f1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UnaANLsK6YJ"
      },
      "outputs": [],
      "source": [
        "model1 = LogisticRegression(penalty='none', C=0.10101010101010101, solver='lbfgs', class_weight={0: 191808.0, 1: 18144.0}).fit(X1_train, y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcNsQMtYK3KK"
      },
      "outputs": [],
      "source": [
        "clf = print_auc(model1, X1_train, y1_train, X1_test, y1_test, 'Logistic Regression')\n",
        "results_df = pd.DataFrame(data=[['Logistic Regression', clf['f1'], clf['auc'], clf['auc_pr'], clf['recall_sc'][0], clf['recall_sc'][1], clf['precision_sc'][0], clf['precision_sc'][1]]], \n",
        "                          columns=['Model', 'f1_score', 'auc', 'auc_pr', 'recall_classe(0)', 'recall_classe(1)', 'precision_classe(0)', 'precision_classe(1)'])\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLKEYo31Y18n"
      },
      "source": [
        "## **SVC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdS3wufBZEJn"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC  \n",
        "\n",
        "clf1 = SVC(probability=True)\n",
        "\n",
        "cross_val = StratifiedKFold(n_splits=5)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5.5, 5.5))\n",
        "\n",
        "tprs, aucs, y_real, y_prob, prs = [], [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "    \n",
        "plt.figure(figsize=(5.5 , 5.5))\n",
        "\n",
        "accs, precs, recs, f1_scr = [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i=0\n",
        "\n",
        "param_grid = {'C': uniform(0.1, 100),\n",
        "              'degree': randint(2, 5),\n",
        "              'gamma': ['scale', 'auto'],\n",
        "              'class_weight': [None, 'balanced', class_weights]}\n",
        "\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "lst_accu_stratified = []\n",
        "\n",
        "index_iterator = cross_val.split(X, y)\n",
        "\n",
        "clf_random = clf_random = RandomizedSearchCV(estimator=clf1, param_distributions=param_grid, n_iter=100, scoring='f1', cv=cross_val, verbose=2, n_jobs=-1)\n",
        "for train_index, test_index in index_iterator:\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "  \n",
        "    clf_random.fit(X_train, y_train)\n",
        "    pred_proba = clf_random.predict_proba(X_test)\n",
        "    \n",
        "    precision, recall, _ = precision_recall_curve(y_test, pred_proba[:, 1])\n",
        "    prs.append(interp(mean_recall, precision, recall))\n",
        "    pr_auc = auc(recall, precision)\n",
        "    aucs.append(pr_auc)\n",
        "    plt.plot(recall, precision, lw=3, alpha=0.5, label='Fold %d (AUCPR = %0.2f)' % (i+1, pr_auc))\n",
        "\n",
        "    viz = RocCurveDisplay.from_estimator(clf_random, X_test, y_test, name='ROC fold {}'.format(i+1), ax=ax, alpha=.3)\n",
        "\n",
        "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(viz.roc_auc)\n",
        "\n",
        "    lst_accu_stratified.append(clf_random.score(X_test, y_test))\n",
        "    y_train_pred = clf_random.predict(X_train)\n",
        "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel())*100\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    y_test_pred = clf_random.predict(X_test)\n",
        "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel())*100\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('Best Score: %s'.format(i) % clf_random.best_score_)\n",
        "    print('Best Hyperparameters: %s'.format(i) % clf_random.best_params_)\n",
        "    acc = accuracy_score(y_test, y_test_pred)\n",
        "    prec = precision_score(y_test, y_test_pred)\n",
        "    rec = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test,y_test_pred)\n",
        "    accs.append(acc)\n",
        "    precs.append(prec)\n",
        "    recs.append(rec)\n",
        "    f1_scr.append(f1)\n",
        "    print(f'Accuracy: {acc}, Precision: {prec}, Recall: {rec}, f1_score: {f1}')\n",
        "    i=i+1\n",
        "\n",
        "\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', lw=.7, color='black', label='Chance', alpha=.7)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "ax.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=.7, alpha=.7)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.7, label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic for LGBM\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot([0, 1], [1, 0], linestyle='--', lw=.7, color='k', label='Luck', alpha=.7)\n",
        "mean_precision = np.mean(prs, axis=0)\n",
        "mean_auc = auc(mean_recall, mean_precision)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_precision, mean_recall, color='navy',label=r'Mean (AUCPR = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=4)\n",
        "    \n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('Recall' ,  fontweight = \"bold\" , fontsize=10)\n",
        "plt.ylabel('Precision',fontweight = \"bold\" , fontsize=10)\n",
        "plt.tick_params(axis='both', which='major', labelsize=10)\n",
        "plt.legend( prop={'size':10} , loc = 0)\n",
        "plt.show()\n",
        "\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "print('List of possible accuracy: \\n', lst_accu_stratified)\n",
        "print('Maximum Accuracy That can be obtained from this model is: \\n', max(lst_accu_stratified)*100, '%')\n",
        "print('Minimum Accuracy: \\n', min(lst_accu_stratified)*100, '%')\n",
        "print('Overall Accuracy: \\n', mean(lst_accu_stratified)*100, '%')\n",
        "print('Standard Deviation is: \\n', stdev(lst_accu_stratified))\n",
        "\n",
        "print('List of possible accuracy for trainning:', train_accuracy_list)\n",
        "print('List of possible accuracy for testing:', test_accuracy_list)\n",
        "\n",
        "print(f'Mean Accuracy: {np.mean(accs)}, Mean Precision: {np.mean(precs)}, Mean Recall: {np.mean(recs)}, Mean f1_score: {np.mean(f1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-keDIf0c1rY"
      },
      "outputs": [],
      "source": [
        "model2 = SVC(C=1, kernel='rbf', gamma=0.001, class_weight='balanced', probability=True).fit(X1_train, y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoGvPg8-dMci"
      },
      "outputs": [],
      "source": [
        "clf = print_auc(model2, X1_train, y1_train, X1_test, y1_test, 'SVM')\n",
        "results_df2 = pd.DataFrame(data=[['SVM', clf['f1'], clf['auc'], clf['auc_pr'], clf['recall_sc'][0], clf['recall_sc'][1], clf['precision_sc'][0], clf['precision_sc'][1]]],  \n",
        "                          columns=['Model', 'f1_score', 'auc','auc_pr','recall_classe(0)','recall_classe(1)','precision_classe(0)','precision_classe(1)'])\n",
        "results_df = results_df.append(results_df2)\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r0M0flGeNgv"
      },
      "source": [
        "## **Gradient Boosting Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JuDN4XUeUMf"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "clf1 = GradientBoostingClassifier()\n",
        "\n",
        "cross_val = StratifiedKFold(n_splits=5)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5.5, 5.5))\n",
        "\n",
        "tprs, aucs, y_real, y_prob, prs = [], [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "    \n",
        "plt.figure(figsize=(5.5 , 5.5))\n",
        "\n",
        "accs, precs, recs, f1_scr = [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i=0\n",
        "\n",
        "param_grid = {'learning_rate': uniform(1e-5, 1),\n",
        "              'subsample': uniform(0.8, 1),\n",
        "              'n_estimators' : randint(100, 1000),\n",
        "              'max_features' : ['log2', 'sqrt', 'auto'],\n",
        "              'max_depth': randint(3, 10)}\n",
        "\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "lst_accu_stratified = []\n",
        "\n",
        "index_iterator = cross_val.split(X, y)\n",
        "\n",
        "clf_random = clf_random = RandomizedSearchCV(estimator=clf1, param_distributions=param_grid, n_iter=100, scoring='f1', cv=cross_val, verbose=2, n_jobs=-1)\n",
        "for train_index, test_index in index_iterator:\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "  \n",
        "    clf_random.fit(X_train, y_train)\n",
        "    pred_proba = clf_random.predict_proba(X_test)\n",
        "    \n",
        "    precision, recall, _ = precision_recall_curve(y_test, pred_proba[:, 1])\n",
        "    prs.append(interp(mean_recall, precision, recall))\n",
        "    pr_auc = auc(recall, precision)\n",
        "    aucs.append(pr_auc)\n",
        "    plt.plot(recall, precision, lw=3, alpha=0.5, label='Fold %d (AUCPR = %0.2f)' % (i+1, pr_auc))\n",
        "\n",
        "    viz = RocCurveDisplay.from_estimator(clf_random, X_test, y_test, name='ROC fold {}'.format(i+1), ax=ax, alpha=.3)\n",
        "\n",
        "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(viz.roc_auc)\n",
        "\n",
        "    lst_accu_stratified.append(clf_random.score(X_test, y_test))\n",
        "    y_train_pred = clf_random.predict(X_train)\n",
        "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel())*100\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    y_test_pred = clf_random.predict(X_test)\n",
        "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel())*100\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('Best Score: %s'.format(i) % clf_random.best_score_)\n",
        "    print('Best Hyperparameters: %s'.format(i) % clf_random.best_params_)\n",
        "    acc = accuracy_score(y_test, y_test_pred)\n",
        "    prec = precision_score(y_test, y_test_pred)\n",
        "    rec = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test,y_test_pred)\n",
        "    accs.append(acc)\n",
        "    precs.append(prec)\n",
        "    recs.append(rec)\n",
        "    f1_scr.append(f1)\n",
        "    print(f'Accuracy: {acc}, Precision: {prec}, Recall: {rec}, f1_score: {f1}')\n",
        "    i=i+1\n",
        "\n",
        "\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', lw=.7, color='black', label='Chance', alpha=.7)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "ax.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=.7, alpha=.7)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.7, label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic for LGBM\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot([0, 1], [1, 0], linestyle='--', lw=.7, color='k', label='Luck', alpha=.7)\n",
        "mean_precision = np.mean(prs, axis=0)\n",
        "mean_auc = auc(mean_recall, mean_precision)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_precision, mean_recall, color='navy',label=r'Mean (AUCPR = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=4)\n",
        "    \n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('Recall' ,  fontweight = \"bold\" , fontsize=10)\n",
        "plt.ylabel('Precision',fontweight = \"bold\" , fontsize=10)\n",
        "plt.tick_params(axis='both', which='major', labelsize=10)\n",
        "plt.legend( prop={'size':10} , loc = 0)\n",
        "plt.show()\n",
        "\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "print('List of possible accuracy: \\n', lst_accu_stratified)\n",
        "print('Maximum Accuracy That can be obtained from this model is: \\n', max(lst_accu_stratified)*100, '%')\n",
        "print('Minimum Accuracy: \\n', min(lst_accu_stratified)*100, '%')\n",
        "print('Overall Accuracy: \\n', mean(lst_accu_stratified)*100, '%')\n",
        "print('Standard Deviation is: \\n', stdev(lst_accu_stratified))\n",
        "\n",
        "print('List of possible accuracy for trainning:', train_accuracy_list)\n",
        "print('List of possible accuracy for testing:', test_accuracy_list)\n",
        "\n",
        "print(f'Mean Accuracy: {np.mean(accs)}, Mean Precision: {np.mean(precs)}, Mean Recall: {np.mean(recs)}, Mean f1_score: {np.mean(f1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UI7zesRgAl2"
      },
      "outputs": [],
      "source": [
        "model3 = GradientBoostingClassifier(learning_rate=0.02 , subsample=0.2, n_estimators=100, max_features='log2', max_depth=8).fit(X1_train, y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mbpk8XlwhKQP"
      },
      "outputs": [],
      "source": [
        "clf = print_auc(model3, X1_train, y1_train, X1_test, y1_test, 'Gradient Boosting')\n",
        "results_df3 = pd.DataFrame(data=[['Gradient Boosting', clf['f1'], clf['auc'], clf['auc_pr'], clf['recall_sc'][0], clf['recall_sc'][1], clf['precision_sc'][0], clf['precision_sc'][1]]], \n",
        "                          columns=['Model', 'f1_score', 'auc','auc_pr','recall_classe(0)','recall_classe(1)','precision_classe(0)','precision_classe(1)'])\n",
        "results_df = results_df.append(results_df3)\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFeYmV1Xp236"
      },
      "source": [
        "## **Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-bD2GOrp1SB"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf1 = RandomForestClassifier(random_state=seed)\n",
        "\n",
        "cross_val = StratifiedKFold(n_splits=5)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5.5, 5.5))\n",
        "\n",
        "tprs, aucs, y_real, y_prob, prs = [], [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "    \n",
        "plt.figure(figsize=(5.5 , 5.5))\n",
        "\n",
        "accs, precs, recs, f1_scr = [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i=0\n",
        "\n",
        "param_grid = {'n_estimators': randint(50, 1000),\n",
        "              'max_depth': randint(3, 10),\n",
        "              'min_samples_split': randint(2, 10),\n",
        "              'min_samples_leaf': randint(1, 5),\n",
        "              'max_features': ['sqrt', 'log2', None],\n",
        "              'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "              'class_weight' : ['balanced', None, class_weights]}\n",
        "\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "lst_accu_stratified = []\n",
        "\n",
        "index_iterator = cross_val.split(X, y)\n",
        "\n",
        "clf_random = clf_random = RandomizedSearchCV(estimator=clf1, param_distributions=param_grid, n_iter=100, scoring='f1', cv=cross_val, verbose=2, n_jobs=-1)\n",
        "for train_index, test_index in index_iterator:\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "  \n",
        "    clf_random.fit(X_train, y_train)\n",
        "    pred_proba = clf_random.predict_proba(X_test)\n",
        "    \n",
        "    precision, recall, _ = precision_recall_curve(y_test, pred_proba[:, 1])\n",
        "    prs.append(interp(mean_recall, precision, recall))\n",
        "    pr_auc = auc(recall, precision)\n",
        "    aucs.append(pr_auc)\n",
        "    plt.plot(recall, precision, lw=3, alpha=0.5, label='Fold %d (AUCPR = %0.2f)' % (i+1, pr_auc))\n",
        "\n",
        "    viz = RocCurveDisplay.from_estimator(clf_random, X_test, y_test, name='ROC fold {}'.format(i+1), ax=ax, alpha=.3)\n",
        "\n",
        "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(viz.roc_auc)\n",
        "\n",
        "    lst_accu_stratified.append(clf_random.score(X_test, y_test))\n",
        "    y_train_pred = clf_random.predict(X_train)\n",
        "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel())*100\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    y_test_pred = clf_random.predict(X_test)\n",
        "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel())*100\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('Best Score: %s'.format(i) % clf_random.best_score_)\n",
        "    print('Best Hyperparameters: %s'.format(i) % clf_random.best_params_)\n",
        "    acc = accuracy_score(y_test, y_test_pred)\n",
        "    prec = precision_score(y_test, y_test_pred)\n",
        "    rec = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test,y_test_pred)\n",
        "    accs.append(acc)\n",
        "    precs.append(prec)\n",
        "    recs.append(rec)\n",
        "    f1_scr.append(f1)\n",
        "    print(f'Accuracy: {acc}, Precision: {prec}, Recall: {rec}, f1_score: {f1}')\n",
        "    i=i+1\n",
        "\n",
        "\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', lw=.7, color='black', label='Chance', alpha=.7)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "ax.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=.7, alpha=.7)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.7, label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic for LGBM\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot([0, 1], [1, 0], linestyle='--', lw=.7, color='k', label='Luck', alpha=.7)\n",
        "mean_precision = np.mean(prs, axis=0)\n",
        "mean_auc = auc(mean_recall, mean_precision)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_precision, mean_recall, color='navy',label=r'Mean (AUCPR = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=4)\n",
        "    \n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('Recall' ,  fontweight = \"bold\" , fontsize=10)\n",
        "plt.ylabel('Precision',fontweight = \"bold\" , fontsize=10)\n",
        "plt.tick_params(axis='both', which='major', labelsize=10)\n",
        "plt.legend( prop={'size':10} , loc = 0)\n",
        "plt.show()\n",
        "\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "print('List of possible accuracy:', lst_accu_stratified)\n",
        "print('\\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified)*100, '%')\n",
        "print('\\nMinimum Accuracy:', min(lst_accu_stratified)*100, '%')\n",
        "print('\\nOverall Accuracy:', mean(lst_accu_stratified)*100, '%')\n",
        "print('\\nStandard Deviation is:', stdev(lst_accu_stratified))\n",
        "\n",
        "print('List of possible accuracy for trainning:', train_accuracy_list)\n",
        "print('List of possible accuracy for testing:', test_accuracy_list)\n",
        "\n",
        "print(f'Mean Accuracy: {np.mean(accs)}, Mean Precision: {np.mean(precs)}, Mean Recall: {np.mean(recs)}, Mean f1_score: {np.mean(f1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_rGjseAqi5S"
      },
      "outputs": [],
      "source": [
        "model4 = RandomForestClassifier(n_estimators=100, max_features='log2', max_depth=30, criterion='entropy').fit(X1_train, y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjAjmpdyqpll"
      },
      "outputs": [],
      "source": [
        "clf = print_auc(model4, X1_train, y1_train, X1_test, y1_test, 'Random Forest')\n",
        "results_df3 = pd.DataFrame(data=[['Random Forest', clf['f1'], clf['auc'], clf['auc_pr'], clf['recall_sc'][0], clf['recall_sc'][1], clf['precision_sc'][0], clf['precision_sc'][1]]], \n",
        "                          columns=['Model', 'f1_score', 'auc', 'auc_pr', 'recall_classe(0)', 'recall_classe(1)', 'precision_classe(0)', 'precision_classe(1)'])\n",
        "results_df = results_df.append(results_df3, ignore_index=True)\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5xG9U11q_xU"
      },
      "source": [
        "## **KNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51Xk1zoLrDTw"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "clf1 = KNeighborsClassifier()\n",
        "\n",
        "cross_val = StratifiedKFold(n_splits=5)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5.5, 5.5))\n",
        "\n",
        "tprs, aucs, y_real, y_prob, prs = [], [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "    \n",
        "plt.figure(figsize=(5.5 , 5.5))\n",
        "\n",
        "accs, precs, recs, f1_scr = [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i=0\n",
        "\n",
        "param_grid = {'n_neighbors': randint(3, 7),\n",
        "              'weights': ['uniform', 'distance'],\n",
        "              'p': [1, 2],\n",
        "              'algorithm': ['brute'],\n",
        "              'leaf_size': randint(10, 50)}\n",
        "\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "lst_accu_stratified = []\n",
        "\n",
        "index_iterator = cross_val.split(X, y)\n",
        "\n",
        "clf_random = clf_random = RandomizedSearchCV(estimator=clf1, param_distributions=param_grid, n_iter=100, scoring='f1', cv=cross_val, verbose=2, n_jobs=-1)\n",
        "\n",
        "for train_index, test_index in index_iterator:\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "  \n",
        "    clf_random.fit(X_train, y_train)\n",
        "    pred_proba = clf_random.predict_proba(X_test)\n",
        "    \n",
        "    precision, recall, _ = precision_recall_curve(y_test, pred_proba[:, 1])\n",
        "    prs.append(interp(mean_recall, precision, recall))\n",
        "    pr_auc = auc(recall, precision)\n",
        "    aucs.append(pr_auc)\n",
        "    plt.plot(recall, precision, lw=3, alpha=0.5, label='Fold %d (AUCPR = %0.2f)' % (i+1, pr_auc))\n",
        "\n",
        "    viz = RocCurveDisplay.from_estimator(clf_random, X_test, y_test, name='ROC fold {}'.format(i+1), ax=ax, alpha=.3)\n",
        "\n",
        "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(viz.roc_auc)\n",
        "\n",
        "    lst_accu_stratified.append(clf_random.score(X_test, y_test))\n",
        "    y_train_pred = clf_random.predict(X_train)\n",
        "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel())*100\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    y_test_pred = clf_random.predict(X_test)\n",
        "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel())*100\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('Best Score: %s'.format(i) % clf_random.best_score_)\n",
        "    print('Best Hyperparameters: %s'.format(i) % clf_random.best_params_)\n",
        "    acc = accuracy_score(y_test, y_test_pred)\n",
        "    prec = precision_score(y_test, y_test_pred)\n",
        "    rec = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test,y_test_pred)\n",
        "    accs.append(acc)\n",
        "    precs.append(prec)\n",
        "    recs.append(rec)\n",
        "    f1_scr.append(f1)\n",
        "    print(f'Accuracy: {acc}, Precision: {prec}, Recall: {rec}, f1_score: {f1}')\n",
        "    i=i+1\n",
        "\n",
        "\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', lw=.7, color='black', label='Chance', alpha=.7)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "ax.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=.7, alpha=.7)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.7, label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic for LGBM\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot([0, 1], [1, 0], linestyle='--', lw=.7, color='k', label='Luck', alpha=.7)\n",
        "mean_precision = np.mean(prs, axis=0)\n",
        "mean_auc = auc(mean_recall, mean_precision)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_precision, mean_recall, color='navy',label=r'Mean (AUCPR = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=4)\n",
        "    \n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('Recall' ,  fontweight = \"bold\" , fontsize=10)\n",
        "plt.ylabel('Precision',fontweight = \"bold\" , fontsize=10)\n",
        "plt.tick_params(axis='both', which='major', labelsize=10)\n",
        "plt.legend( prop={'size':10} , loc = 0)\n",
        "plt.show()\n",
        "\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "print('List of possible accuracy: \\n', lst_accu_stratified)\n",
        "print('Maximum Accuracy That can be obtained from this model is: \\n', max(lst_accu_stratified)*100, '%')\n",
        "print('Minimum Accuracy: \\n', min(lst_accu_stratified)*100, '%')\n",
        "print('Overall Accuracy: \\n', mean(lst_accu_stratified)*100, '%')\n",
        "print('Standard Deviation is: \\n', stdev(lst_accu_stratified))\n",
        "\n",
        "print('List of possible accuracy for trainning:', train_accuracy_list)\n",
        "print('List of possible accuracy for testing:', test_accuracy_list)\n",
        "\n",
        "print(f'Mean Accuracy: {np.mean(accs)}, Mean Precision: {np.mean(precs)}, Mean Recall: {np.mean(recs)}, Mean f1_score: {np.mean(f1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqEMiUaBraUs"
      },
      "outputs": [],
      "source": [
        "model5 = KNeighborsClassifier(n_neighbors=11, weights='uniform', metric='euclidean').fit(X1_train, y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSJWhF2zrvcU"
      },
      "outputs": [],
      "source": [
        "clf = print_auc(model5, X1_train, y1_train, X1_test, y1_test, 'KNeibors Classifier')\n",
        "results_df4 = pd.DataFrame(data=[['KNeibors Classifier', clf['f1'], clf['auc'], clf['auc_pr'], clf['recall_sc'][0], clf['recall_sc'][1], clf['precision_sc'][0], clf['precision_sc'][1]]], \n",
        "                          columns=['Model', 'f1_score', 'auc', 'auc_pr', 'recall_classe(0)', 'recall_classe(1)', 'precision_classe(0)', 'precision_classe(1)'])\n",
        "results_df = results_df.append(results_df4, ignore_index=True)\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syq3lk7pr8ge"
      },
      "source": [
        "## **ExtraTrees**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWKqryuSsHTY"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "clf1 = ExtraTreesClassifier(random_state=0)\n",
        "\n",
        "cross_val = StratifiedKFold(n_splits=5)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5.5, 5.5))\n",
        "\n",
        "tprs, aucs, y_real, y_prob, prs = [], [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "    \n",
        "plt.figure(figsize=(5.5 , 5.5))\n",
        "\n",
        "accs, precs, recs, f1_scr = [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i=0\n",
        "\n",
        "param_grid = {'n_estimators': randint(100, 1000),\n",
        "              'max_depth': randint(10, 100),\n",
        "              'max_features': ['sqrt', 'log2'],\n",
        "              'min_samples_split' : randint(2, 10), \n",
        "              'min_samples_leaf' : randint(1, 10),\n",
        "              'class_weight' : ['balanced', None, class_weights]}\n",
        "\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "lst_accu_stratified = []\n",
        "\n",
        "index_iterator = cross_val.split(X, y)\n",
        "\n",
        "clf_random = clf_random = RandomizedSearchCV(estimator=clf1, param_distributions=param_grid, n_iter=100, scoring='f1', cv=cross_val, verbose=2, n_jobs=-1)\n",
        "for train_index, test_index in index_iterator:\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "  \n",
        "    clf_random.fit(X_train, y_train)\n",
        "    pred_proba = clf_random.predict_proba(X_test)\n",
        "    \n",
        "    precision, recall, _ = precision_recall_curve(y_test, pred_proba[:, 1])\n",
        "    prs.append(interp(mean_recall, precision, recall))\n",
        "    pr_auc = auc(recall, precision)\n",
        "    aucs.append(pr_auc)\n",
        "    plt.plot(recall, precision, lw=3, alpha=0.5, label='Fold %d (AUCPR = %0.2f)' % (i+1, pr_auc))\n",
        "\n",
        "    viz = RocCurveDisplay.from_estimator(clf_random, X_test, y_test, name='ROC fold {}'.format(i+1), ax=ax, alpha=.3)\n",
        "\n",
        "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(viz.roc_auc)\n",
        "\n",
        "    lst_accu_stratified.append(clf_random.score(X_test, y_test))\n",
        "    y_train_pred = clf_random.predict(X_train)\n",
        "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel())*100\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    y_test_pred = clf_random.predict(X_test)\n",
        "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel())*100\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "    \n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('Best Score: %s'.format(i) % clf_random.best_score_)\n",
        "    print('Best Hyperparameters: %s'.format(i) % clf_random.best_params_)\n",
        "    acc = accuracy_score(y_test, y_test_pred)\n",
        "    prec = precision_score(y_test, y_test_pred)\n",
        "    rec = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test,y_test_pred)\n",
        "    accs.append(acc)\n",
        "    precs.append(prec)\n",
        "    recs.append(rec)\n",
        "    f1_scr.append(f1)\n",
        "    print(f'Accuracy: {acc}, Precision: {prec}, Recall: {rec}, f1_score: {f1}')\n",
        "    i=i+1\n",
        "\n",
        "\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', lw=.7, color='black', label='Chance', alpha=.7)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "ax.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=.7, alpha=.7)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.7, label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic for LGBM\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot([0, 1], [1, 0], linestyle='--', lw=.7, color='k', label='Luck', alpha=.7)\n",
        "mean_precision = np.mean(prs, axis=0)\n",
        "mean_auc = auc(mean_recall, mean_precision)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_precision, mean_recall, color='navy',label=r'Mean (AUCPR = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=4)\n",
        "    \n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('Recall' ,  fontweight = \"bold\" , fontsize=10)\n",
        "plt.ylabel('Precision',fontweight = \"bold\" , fontsize=10)\n",
        "plt.tick_params(axis='both', which='major', labelsize=10)\n",
        "plt.legend( prop={'size':10} , loc = 0)\n",
        "plt.show()\n",
        "\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "print('List of possible accuracy: \\n', lst_accu_stratified)\n",
        "print('Maximum Accuracy That can be obtained from this model is: \\n', max(lst_accu_stratified)*100, '%')\n",
        "print('Minimum Accuracy: \\n', min(lst_accu_stratified)*100, '%')\n",
        "print('Overall Accuracy: \\n', mean(lst_accu_stratified)*100, '%')\n",
        "print('Standard Deviation is: \\n', stdev(lst_accu_stratified))\n",
        "\n",
        "print('List of possible accuracy for trainning:', train_accuracy_list)\n",
        "print('List of possible accuracy for testing:', test_accuracy_list)\n",
        "\n",
        "print(f'Mean Accuracy: {np.mean(accs)}, Mean Precision: {np.mean(precs)}, Mean Recall: {np.mean(recs)}, Mean f1_score: {np.mean(f1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_Sw0d2Wsll-"
      },
      "outputs": [],
      "source": [
        "model6 = ExtraTreesClassifier(random_state=1, n_estimators=320, max_depth=25).fit(X1_train, y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv68O4tJsZaR"
      },
      "outputs": [],
      "source": [
        "clf = print_auc(model6, X1_train, y1_train, X1_test, y1_test, 'ExtraTrees Classifier')\n",
        "results_df5 = pd.DataFrame(data=[['ExtraTrees Classifier', clf['f1'], clf['auc'], clf['auc_pr'], clf['recall_sc'][0], clf['recall_sc'][1], clf['precision_sc'][0], clf['precision_sc'][1]]], \n",
        "                          columns=['Model', 'f1_score', 'auc', 'auc_pr', 'recall_classe(0)', 'recall_classe(1)', 'precision_classe(0)', 'precision_classe(1)'])\n",
        "results_df = results_df.append(results_df5, ignore_index=True)\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDVKw76Isvpd"
      },
      "source": [
        "## **AdaBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VioN1uWmsu1N"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf1 = AdaBoostClassifier(random_state=0)\n",
        "\n",
        "cross_val = StratifiedKFold(n_splits=5)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5.5, 5.5))\n",
        "\n",
        "tprs, aucs, y_real, y_prob, prs = [], [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "    \n",
        "plt.figure(figsize=(5.5 , 5.5))\n",
        "\n",
        "accs, precs, recs, f1_scr = [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i=0\n",
        "\n",
        "param_grid = {'n_estimators': randint(50, 200),\n",
        "              'learning_rate': uniform(1e-5, 1),\n",
        "              'base_estimator': [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=3), None]}\n",
        "\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "lst_accu_stratified = []\n",
        "\n",
        "index_iterator = cross_val.split(X, y)\n",
        "\n",
        "clf_random = clf_random = RandomizedSearchCV(estimator=clf1, param_distributions=param_grid, n_iter=100, scoring='f1', cv=cross_val, verbose=2, n_jobs=-1)\n",
        "for train_index, test_index in index_iterator:\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "  \n",
        "    clf_random.fit(X_train, y_train)\n",
        "    pred_proba = clf_random.predict_proba(X_test)\n",
        "    \n",
        "    precision, recall, _ = precision_recall_curve(y_test, pred_proba[:, 1])\n",
        "    prs.append(interp(mean_recall, precision, recall))\n",
        "    pr_auc = auc(recall, precision)\n",
        "    aucs.append(pr_auc)\n",
        "    plt.plot(recall, precision, lw=3, alpha=0.5, label='Fold %d (AUCPR = %0.2f)' % (i+1, pr_auc))\n",
        "\n",
        "    viz = RocCurveDisplay.from_estimator(clf_random, X_test, y_test, name='ROC fold {}'.format(i+1), ax=ax, alpha=.3)\n",
        "\n",
        "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(viz.roc_auc)\n",
        "\n",
        "    lst_accu_stratified.append(clf_random.score(X_test, y_test))\n",
        "    y_train_pred = clf_random.predict(X_train)\n",
        "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel())*100\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    y_test_pred = clf_random.predict(X_test)\n",
        "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel())*100\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('Best Score: %s'.format(i) % clf_random.best_score_)\n",
        "    print('Best Hyperparameters: %s'.format(i) % clf_random.best_params_)\n",
        "    acc = accuracy_score(y_test, y_test_pred)\n",
        "    prec = precision_score(y_test, y_test_pred)\n",
        "    rec = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test,y_test_pred)\n",
        "    accs.append(acc)\n",
        "    precs.append(prec)\n",
        "    recs.append(rec)\n",
        "    f1_scr.append(f1)\n",
        "    print(f'Accuracy: {acc}, Precision: {prec}, Recall: {rec}, f1_score: {f1}')\n",
        "    i=i+1\n",
        "\n",
        "\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', lw=.7, color='black', label='Chance', alpha=.7)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "ax.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=.7, alpha=.7)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.7, label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic for LGBM\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot([0, 1], [1, 0], linestyle='--', lw=.7, color='k', label='Luck', alpha=.7)\n",
        "mean_precision = np.mean(prs, axis=0)\n",
        "mean_auc = auc(mean_recall, mean_precision)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_precision, mean_recall, color='navy',label=r'Mean (AUCPR = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=4)\n",
        "    \n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('Recall' ,  fontweight = \"bold\" , fontsize=10)\n",
        "plt.ylabel('Precision',fontweight = \"bold\" , fontsize=10)\n",
        "plt.tick_params(axis='both', which='major', labelsize=10)\n",
        "plt.legend( prop={'size':10} , loc = 0)\n",
        "plt.show()\n",
        "\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "print('List of possible accuracy: \\n', lst_accu_stratified)\n",
        "print('Maximum Accuracy That can be obtained from this model is \\n max(lst_accu_stratified)*100, '%')\n",
        "print('Minimum Accuracy: \\n', min(lst_accu_stratified)*100, '%')\n",
        "print('Overall Accuracy: \\n', mean(lst_accu_stratified)*100, '%')\n",
        "print('Standard Deviation is: \\n', stdev(lst_accu_stratified))\n",
        "\n",
        "print('List of possible accuracy for trainning:', train_accuracy_list)\n",
        "print('List of possible accuracy for testing:', test_accuracy_list)\n",
        "\n",
        "print(f'Mean Accuracy: {np.mean(accs)}, Mean Precision: {np.mean(precs)}, Mean Recall: {np.mean(recs)}, Mean f1_score: {np.mean(f1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2P3wCJDtMrU"
      },
      "outputs": [],
      "source": [
        "model7 = AdaBoostClassifier(algorithm='SAMME.R', n_estimators=1000, learning_rate=0.1).fit(X1_train, y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnUmGgS2tNUh"
      },
      "outputs": [],
      "source": [
        "clf = print_auc(model7, X1_train, y1_train, X1_test, y1_test, 'AdaBoost Classifier')\n",
        "results_df6 = pd.DataFrame(data=[['AdaBoost Classifier', clf['f1'], clf['auc'], clf['auc_pr'], clf['recall_sc'][0], clf['recall_sc'][1], clf['precision_sc'][0], clf['precision_sc'][1]]], \n",
        "                          columns=['Model', 'f1_score', 'auc', 'auc_pr', 'recall_classe(0)', 'recall_classe(1)', 'precision_classe(0)', 'precision_classe(1)'])\n",
        "results_df = results_df.append(results_df6, ignore_index=True)\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efDjm1dBtfZz"
      },
      "source": [
        "## **Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48rr8bz2toDh"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf1 = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "cross_val = StratifiedKFold(n_splits=5)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5.5, 5.5))\n",
        "\n",
        "tprs, aucs, y_real, y_prob, prs = [], [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "    \n",
        "plt.figure(figsize=(5.5 , 5.5))\n",
        "\n",
        "accs, precs, recs, f1_scr = [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i=0\n",
        "\n",
        "param_grid = {'min_samples_split': randint(2, 20),\n",
        "              'max_depth': randint(3, 10),\n",
        "              'min_samples_leaf': randint(2, 10),\n",
        "              'criterion' : ['gini', 'entropy', 'log_loss'],\n",
        "              'max_features': ['auto', 'sqrt', 'log2']}\n",
        "\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "lst_accu_stratified = []\n",
        "\n",
        "index_iterator = cross_val.split(X, y)\n",
        "\n",
        "clf_random = clf_random = RandomizedSearchCV(estimator=clf1, param_distributions=param_grid, n_iter=100, scoring='f1', cv=cross_val, verbose=2, n_jobs=-1)\n",
        "for train_index, test_index in index_iterator:\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "  \n",
        "    clf_random.fit(X_train, y_train)\n",
        "    pred_proba = clf_random.predict_proba(X_test)\n",
        "    \n",
        "    precision, recall, _ = precision_recall_curve(y_test, pred_proba[:, 1])\n",
        "    prs.append(interp(mean_recall, precision, recall))\n",
        "    pr_auc = auc(recall, precision)\n",
        "    aucs.append(pr_auc)\n",
        "    plt.plot(recall, precision, lw=3, alpha=0.5, label='Fold %d (AUCPR = %0.2f)' % (i+1, pr_auc))\n",
        "\n",
        "    viz = RocCurveDisplay.from_estimator(clf_random, X_test, y_test, name='ROC fold {}'.format(i+1), ax=ax, alpha=.3)\n",
        "\n",
        "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(viz.roc_auc)\n",
        "\n",
        "    lst_accu_stratified.append(clf_random.score(X_test, y_test))\n",
        "    y_train_pred = clf_random.predict(X_train)\n",
        "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel())*100\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    y_test_pred = clf_random.predict(X_test)\n",
        "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel())*100\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('Best Score: %s'.format(i) % clf_random.best_score_)\n",
        "    print('Best Hyperparameters: %s'.format(i) % clf_random.best_params_)\n",
        "    acc = accuracy_score(y_test, y_test_pred)\n",
        "    prec = precision_score(y_test, y_test_pred)\n",
        "    rec = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test,y_test_pred)\n",
        "    accs.append(acc)\n",
        "    precs.append(prec)\n",
        "    recs.append(rec)\n",
        "    f1_scr.append(f1)\n",
        "    print(f'Accuracy: {acc}, Precision: {prec}, Recall: {rec}, f1_score: {f1}')\n",
        "    i=i+1\n",
        "\n",
        "\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', lw=.7, color='black', label='Chance', alpha=.7)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "ax.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=.7, alpha=.7)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.7, label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic for LGBM\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot([0, 1], [1, 0], linestyle='--', lw=.7, color='k', label='Luck', alpha=.7)\n",
        "mean_precision = np.mean(prs, axis=0)\n",
        "mean_auc = auc(mean_recall, mean_precision)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_precision, mean_recall, color='navy',label=r'Mean (AUCPR = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=4)\n",
        "    \n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('Recall' ,  fontweight = \"bold\" , fontsize=10)\n",
        "plt.ylabel('Precision',fontweight = \"bold\" , fontsize=10)\n",
        "plt.tick_params(axis='both', which='major', labelsize=10)\n",
        "plt.legend( prop={'size':10} , loc = 0)\n",
        "plt.show()\n",
        "\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "print('List of possible accuracy: \\n', lst_accu_stratified)\n",
        "print('Maximum Accuracy That can be obtained from this model is: \\n', max(lst_accu_stratified)*100, '%')\n",
        "print('Minimum Accuracy: \\n', min(lst_accu_stratified)*100, '%')\n",
        "print('Overall Accuracy: \\n', mean(lst_accu_stratified)*100, '%')\n",
        "print('Standard Deviation is: \\n', stdev(lst_accu_stratified))\n",
        "\n",
        "print('List of possible accuracy for trainning:', train_accuracy_list)\n",
        "print('List of possible accuracy for testing:', test_accuracy_list)\n",
        "\n",
        "print(f'Mean Accuracy: {np.mean(accs)}, Mean Precision: {np.mean(precs)}, Mean Recall: {np.mean(recs)}, Mean f1_score: {np.mean(f1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veO8tTDJuJR9"
      },
      "outputs": [],
      "source": [
        "model8 = DecisionTreeClassifier(min_samples_leaf=20, min_samples_split=3, max_depth=5, criterion='entropy').fit(X1_train, y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GaUvPJ4t7Xd"
      },
      "outputs": [],
      "source": [
        "clf = print_auc(model8, X1_train, y1_train, X1_test, y1_test, 'DecisionTree Classifier')\n",
        "results_df7 = pd.DataFrame(data=[['DecisionTree Classifier', clf['f1'], clf['auc'], clf['auc_pr'], clf['recall_sc'][0], clf['recall_sc'][1], clf['precision_sc'][0], clf['precision_sc'][1]]], \n",
        "                          columns=['Model', 'f1_score', 'auc', 'auc_pr', 'recall_classe(0)', 'recall_classe(1)', 'precision_classe(0)', 'precision_classe(1)'])\n",
        "results_df = results_df.append(results_df7, ignore_index=True)\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWaEjF1JuTGW"
      },
      "source": [
        "## **GaussianNB**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58Lk3OpnucLZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "clf1 = GaussianNB()\n",
        "\n",
        "cross_val = StratifiedKFold(n_splits=5)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5.5, 5.5))\n",
        "\n",
        "tprs, aucs, y_real, y_prob, prs = [], [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "    \n",
        "plt.figure(figsize=(5.5 , 5.5))\n",
        "\n",
        "accs, precs, recs, f1_scr = [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i=0\n",
        "\n",
        "param_grid = {'var_smoothing': uniform(1e-9, 1e-6)}\n",
        "\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "lst_accu_stratified = []\n",
        "\n",
        "index_iterator = cross_val.split(X, y)\n",
        "\n",
        "clf_random = clf_random = RandomizedSearchCV(estimator=clf1, param_distributions=param_grid, n_iter=100, scoring='f1', cv=cross_val, verbose=2, n_jobs=-1)\n",
        "for train_index, test_index in index_iterator:\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "  \n",
        "    clf_random.fit(X_train, y_train)\n",
        "    pred_proba = clf_random.predict_proba(X_test)\n",
        "    \n",
        "    precision, recall, _ = precision_recall_curve(y_test, pred_proba[:, 1])\n",
        "    prs.append(interp(mean_recall, precision, recall))\n",
        "    pr_auc = auc(recall, precision)\n",
        "    aucs.append(pr_auc)\n",
        "    plt.plot(recall, precision, lw=3, alpha=0.5, label='Fold %d (AUCPR = %0.2f)' % (i+1, pr_auc))\n",
        "\n",
        "    viz = RocCurveDisplay.from_estimator(clf_random, X_test, y_test, name='ROC fold {}'.format(i+1), ax=ax, alpha=.3)\n",
        "\n",
        "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(viz.roc_auc)\n",
        "\n",
        "    lst_accu_stratified.append(clf_random.score(X_test, y_test))\n",
        "    y_train_pred = clf_random.predict(X_train)\n",
        "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel())*100\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    y_test_pred = clf_random.predict(X_test)\n",
        "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel())*100\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('Best Score: %s'.format(i) % clf_random.best_score_)\n",
        "    print('Best Hyperparameters: %s'.format(i) % clf_random.best_params_)\n",
        "    acc = accuracy_score(y_test, y_test_pred)\n",
        "    prec = precision_score(y_test, y_test_pred)\n",
        "    rec = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test,y_test_pred)\n",
        "    accs.append(acc)\n",
        "    precs.append(prec)\n",
        "    recs.append(rec)\n",
        "    f1_scr.append(f1)\n",
        "    print(f'Accuracy: {acc}, Precision: {prec}, Recall: {rec}, f1_score: {f1}')\n",
        "    i=i+1\n",
        "\n",
        "\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', lw=.7, color='black', label='Chance', alpha=.7)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "ax.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=.7, alpha=.7)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.7, label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic for LGBM\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot([0, 1], [1, 0], linestyle='--', lw=.7, color='k', label='Luck', alpha=.7)\n",
        "mean_precision = np.mean(prs, axis=0)\n",
        "mean_auc = auc(mean_recall, mean_precision)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_precision, mean_recall, color='navy',label=r'Mean (AUCPR = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=4)\n",
        "    \n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('Recall' ,  fontweight = \"bold\" , fontsize=10)\n",
        "plt.ylabel('Precision',fontweight = \"bold\" , fontsize=10)\n",
        "plt.tick_params(axis='both', which='major', labelsize=10)\n",
        "plt.legend( prop={'size':10} , loc = 0)\n",
        "plt.show()\n",
        "\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "print('List of possible accuracy: \\n', lst_accu_stratified)\n",
        "print('Maximum Accuracy That can be obtained from this model is: \\n', max(lst_accu_stratified)*100, '%')\n",
        "print('Minimum Accuracy: \\n', min(lst_accu_stratified)*100, '%')\n",
        "print('Overall Accuracy: \\n', mean(lst_accu_stratified)*100, '%')\n",
        "print('Standard Deviation is: \\n', stdev(lst_accu_stratified))\n",
        "\n",
        "print('List of possible accuracy for trainning:', train_accuracy_list)\n",
        "print('List of possible accuracy for testing:', test_accuracy_list)\n",
        "\n",
        "print(f'Mean Accuracy: {np.mean(accs)}, Mean Precision: {np.mean(precs)}, Mean Recall: {np.mean(recs)}, Mean f1_score: {np.mean(f1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe8yxKLcuzmc"
      },
      "outputs": [],
      "source": [
        "model9 = GaussianNB(var_smoothing=0.1).fit(X1_train, y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7baiNO28up42"
      },
      "outputs": [],
      "source": [
        "clf = print_auc(model9, X1_train, y1_train, X1_test, y1_test, 'GaussianNB')\n",
        "results_df8 = pd.DataFrame(data=[['GaussianNB', clf['f1'], clf['auc'], clf['auc_pr'], clf['recall_sc'][0], clf['recall_sc'][1], clf['precision_sc'][0], clf['precision_sc'][1]]], \n",
        "                          columns=['Model', 'f1_score', 'auc', 'auc_pr', 'recall_classe(0)', 'recall_classe(1)', 'precision_classe(0)', 'precision_classe(1)'])\n",
        "results_df = results_df.append(results_df8, ignore_index=True)\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVPPzb2fu3Vx"
      },
      "source": [
        "## **LGBM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNrWQQLLvNc5"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "clf1 = lgb.LGBMClassifier()\n",
        "\n",
        "cross_val = StratifiedKFold(n_splits=5)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5.5, 5.5))\n",
        "\n",
        "tprs, aucs, y_real, y_prob, prs = [], [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "    \n",
        "plt.figure(figsize=(5.5 , 5.5))\n",
        "\n",
        "accs, precs, recs, f1_scr = [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i=0\n",
        "\n",
        "param_grid = {'num_leaves': randint(10, 30),\n",
        "              'max_depth': randint(3, 10),\n",
        "              'learning_rate': uniform(1e-5, 1),\n",
        "              'n_estimators': randint(50, 200),\n",
        "              'min_child_samples': randint(10, 30),\n",
        "              'subsample': uniform(0.8, 1),\n",
        "              'colsample_bytree': uniform(0.8, 1),\n",
        "              'class_weight' : ['balanced', None, class_weights]}\n",
        "\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "lst_accu_stratified = []\n",
        "\n",
        "index_iterator = cross_val.split(X, y)\n",
        "\n",
        "clf_random = clf_random = RandomizedSearchCV(estimator=clf1, param_distributions=param_grid, n_iter=100, scoring='f1', cv=cross_val, verbose=2, n_jobs=-1)\n",
        "for train_index, test_index in index_iterator:\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "  \n",
        "    clf_random.fit(X_train, y_train)\n",
        "    pred_proba = clf_random.predict_proba(X_test)\n",
        "    \n",
        "    precision, recall, _ = precision_recall_curve(y_test, pred_proba[:, 1])\n",
        "    prs.append(interp(mean_recall, precision, recall))\n",
        "    pr_auc = auc(recall, precision)\n",
        "    aucs.append(pr_auc)\n",
        "    plt.plot(recall, precision, lw=3, alpha=0.5, label='Fold %d (AUCPR = %0.2f)' % (i+1, pr_auc))\n",
        "\n",
        "    viz = RocCurveDisplay.from_estimator(clf_random, X_test, y_test, name='ROC fold {}'.format(i+1), ax=ax, alpha=.3)\n",
        "\n",
        "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(viz.roc_auc)\n",
        "\n",
        "    lst_accu_stratified.append(clf_random.score(X_test, y_test))\n",
        "    y_train_pred = clf_random.predict(X_train)\n",
        "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel())*100\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    y_test_pred = clf_random.predict(X_test)\n",
        "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel())*100\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('Best Score: %s'.format(i) % clf_random.best_score_)\n",
        "    print('Best Hyperparameters: %s'.format(i) % clf_random.best_params_)\n",
        "    acc = accuracy_score(y_test, y_test_pred)\n",
        "    prec = precision_score(y_test, y_test_pred)\n",
        "    rec = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test,y_test_pred)\n",
        "    accs.append(acc)\n",
        "    precs.append(prec)\n",
        "    recs.append(rec)\n",
        "    f1_scr.append(f1)\n",
        "    print(f'Accuracy: {acc}, Precision: {prec}, Recall: {rec}, f1_score: {f1}')\n",
        "    i=i+1\n",
        "\n",
        "\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', lw=.7, color='black', label='Chance', alpha=.7)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "ax.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=.7, alpha=.7)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.7, label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic for LGBM\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot([0, 1], [1, 0], linestyle='--', lw=.7, color='k', label='Luck', alpha=.7)\n",
        "mean_precision = np.mean(prs, axis=0)\n",
        "mean_auc = auc(mean_recall, mean_precision)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_precision, mean_recall, color='navy',label=r'Mean (AUCPR = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=4)\n",
        "    \n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('Recall' ,  fontweight = \"bold\" , fontsize=10)\n",
        "plt.ylabel('Precision',fontweight = \"bold\" , fontsize=10)\n",
        "plt.tick_params(axis='both', which='major', labelsize=10)\n",
        "plt.legend( prop={'size':10} , loc = 0)\n",
        "plt.show()\n",
        "\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "print('List of possible accuracy: \\n', lst_accu_stratified)\n",
        "print('Maximum Accuracy That can be obtained from this model is: \\n', max(lst_accu_stratified)*100, '%')\n",
        "print('Minimum Accuracy: \\n', min(lst_accu_stratified)*100, '%')\n",
        "print('Overall Accuracy: \\n', mean(lst_accu_stratified)*100, '%')\n",
        "print('Standard Deviation is: \\n', stdev(lst_accu_stratified))\n",
        "\n",
        "print('List of possible accuracy for trainning:', train_accuracy_list)\n",
        "print('List of possible accuracy for testing:', test_accuracy_list)\n",
        "\n",
        "print(f'Mean Accuracy: {np.mean(accs)}, Mean Precision: {np.mean(precs)}, Mean Recall: {np.mean(recs)}, Mean f1_score: {np.mean(f1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw-Qi6uavE0t"
      },
      "outputs": [],
      "source": [
        "model10 = lgb.LGBMClassifier(num_leaves=31, reg_alpha=0.1, lambda_l1=0, lambda_l2=0, min_data_in_leaf=100 ).fit(X1_train, y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyGbhmrEu7Lr"
      },
      "outputs": [],
      "source": [
        "clf = print_auc(model10, X1_train, y1_train, X1_test, y1_test, 'LGBN')\n",
        "results_df9 = pd.DataFrame(data=[['LGBM', clf['f1'], clf['auc'], clf['auc_pr'], clf['recall_sc'][0], clf['recall_sc'][1], clf['precision_sc'][0], clf['precision_sc'][1]]], \n",
        "                          columns=['Model', 'f1_score', 'auc', 'auc_pr', 'recall_classe(0)', 'recall_classe(1)', 'precision_classe(0)', 'precision_classe(1)'])\n",
        "results_df = results_df.append(results_df9, ignore_index=True)\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zABwFvKFvgno"
      },
      "source": [
        "## **XGB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhfKa_yLvj0-"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "clf1 = xgb.XGBClassifier(objective='binary:logistic', scale_pos_weight=imbalance_ratio)\n",
        "\n",
        "cross_val = StratifiedKFold(n_splits=5)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5.5, 5.5))\n",
        "\n",
        "tprs, aucs, y_real, y_prob, prs = [], [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "    \n",
        "plt.figure(figsize=(5.5 , 5.5))\n",
        "\n",
        "accs, precs, recs, f1_scr = [], [], [], []\n",
        "\n",
        "mean_recall = np.linspace(0, 1, 100)\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "i=0\n",
        "\n",
        "param_grid = {'learning_rate' : uniform(1e-5, 1),\n",
        "              'max_depth' : randint(3, 25),\n",
        "              'gamma' : uniform(0.1, 1),\n",
        "              'subsample': uniform(0.5, 1),\n",
        "              'colsample_bytree': uniform(0.5, 1),\n",
        "              'scale_pos_weight': [1, imbalance_ratio]}\n",
        "\n",
        "train_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "lst_accu_stratified = []\n",
        "\n",
        "index_iterator = cross_val.split(X, y)\n",
        "\n",
        "clf_random = clf_random = RandomizedSearchCV(estimator=clf1, param_distributions=param_grid, n_iter=100, scoring='f1', cv=cross_val, verbose=2, n_jobs=-1)\n",
        "for train_index, test_index in index_iterator:\n",
        "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
        "  \n",
        "    clf_random.fit(X_train, y_train)\n",
        "    pred_proba = clf_random.predict_proba(X_test)\n",
        "    \n",
        "    precision, recall, _ = precision_recall_curve(y_test, pred_proba[:, 1])\n",
        "    prs.append(interp(mean_recall, precision, recall))\n",
        "    pr_auc = auc(recall, precision)\n",
        "    aucs.append(pr_auc)\n",
        "    plt.plot(recall, precision, lw=3, alpha=0.5, label='Fold %d (AUCPR = %0.2f)' % (i+1, pr_auc))\n",
        "\n",
        "    viz = RocCurveDisplay.from_estimator(clf_random, X_test, y_test, name='ROC fold {}'.format(i+1), ax=ax, alpha=.3)\n",
        "\n",
        "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
        "    interp_tpr[0] = 0.0\n",
        "    tprs.append(interp_tpr)\n",
        "    aucs.append(viz.roc_auc)\n",
        "\n",
        "    lst_accu_stratified.append(clf_random.score(X_test, y_test))\n",
        "    y_train_pred = clf_random.predict(X_train)\n",
        "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel())*100\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    y_test_pred = clf_random.predict(X_test)\n",
        "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel())*100\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('Best Score: %s'.format(i) % clf_random.best_score_)\n",
        "    print('Best Hyperparameters: %s'.format(i) % clf_random.best_params_)\n",
        "    acc = accuracy_score(y_test, y_test_pred)\n",
        "    prec = precision_score(y_test, y_test_pred)\n",
        "    rec = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test,y_test_pred)\n",
        "    accs.append(acc)\n",
        "    precs.append(prec)\n",
        "    recs.append(rec)\n",
        "    f1_scr.append(f1)\n",
        "    print(f'Accuracy: {acc}, Precision: {prec}, Recall: {rec}, f1_score: {f1}')\n",
        "    i=i+1\n",
        "\n",
        "\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', lw=.7, color='black', label='Chance', alpha=.7)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "ax.plot(mean_fpr, mean_tpr, color='blue', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=.7, alpha=.7)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.7, label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic for LGBM\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot([0, 1], [1, 0], linestyle='--', lw=.7, color='k', label='Luck', alpha=.7)\n",
        "mean_precision = np.mean(prs, axis=0)\n",
        "mean_auc = auc(mean_recall, mean_precision)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_precision, mean_recall, color='navy',label=r'Mean (AUCPR = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=4)\n",
        "    \n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('Recall' ,  fontweight = \"bold\" , fontsize=10)\n",
        "plt.ylabel('Precision',fontweight = \"bold\" , fontsize=10)\n",
        "plt.tick_params(axis='both', which='major', labelsize=10)\n",
        "plt.legend( prop={'size':10} , loc = 0)\n",
        "plt.show()\n",
        "\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "print('List of possible accuracy: \\n', lst_accu_stratified)\n",
        "print('Maximum Accuracy That can be obtained from this model is: \\n', max(lst_accu_stratified)*100, '%')\n",
        "print('Minimum Accuracy: \\n', min(lst_accu_stratified)*100, '%')\n",
        "print('Overall Accuracy: \\n', mean(lst_accu_stratified)*100, '%')\n",
        "print('Standard Deviation is: \\n', stdev(lst_accu_stratified))\n",
        "\n",
        "print('List of possible accuracy for trainning:', train_accuracy_list)\n",
        "print('List of possible accuracy for testing:', test_accuracy_list)\n",
        "\n",
        "print(f'Mean Accuracy: {np.mean(accs)}, Mean Precision: {np.mean(precs)}, Mean Recall: {np.mean(recs)}, Mean f1_score: {np.mean(f1)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_PxjO01wyaa"
      },
      "outputs": [],
      "source": [
        "model11 = xgb.XGBClassifier(objective='binary:logistic', scale_pos_weight=imbalance_ratio, learning_rate=, max_depth=, subsample=, colsample_bytree=, reg_alpha=, reg_lambda=).fit(X1_train, y1_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljLl5Wiswmn_"
      },
      "outputs": [],
      "source": [
        "clf = print_auc(model11, X1_train, y1_train, X1_test, y1_test, 'XGB Classifer')\n",
        "results_df10 = pd.DataFrame(data=[['XGB Classifer', clf['f1'], clf['auc'], clf['auc_pr'], clf['recall_sc'][0], clf['recall_sc'][1], clf['precision_sc'][0], clf['precision_sc'][1]]], \n",
        "                          columns=['Model', 'f1_score', 'auc', 'auc_pr', 'recall_classe(0)', 'recall_classe(1)', 'precision_classe(0)', 'precision_classe(1)'])\n",
        "results_df = results_df.append(results_df10, ignore_index=True)\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0bBHzo6yHmv"
      },
      "outputs": [],
      "source": [
        "def plot_feature_importances(clf, X_train, y_train=None, top_n=10, figsize=(5.5, 5.5), print_table=False, title=\"Feature Importances\"):\n",
        "    '''\n",
        "    plot feature importances of a tree-based sklearn estimator\n",
        "    \n",
        "    Note: X_train and y_train are pandas DataFrames\n",
        "    \n",
        "    Note: Scikit-plot is a lovely package but I sometimes have issues\n",
        "              1. flexibility/extendibility\n",
        "              2. complicated models/datasets\n",
        "          But for many situations Scikit-plot is the way to go\n",
        "          see https://scikit-plot.readthedocs.io/en/latest/Quickstart.html\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "        clf         (sklearn estimator) if not fitted, this routine will fit it\n",
        "        \n",
        "        X_train     (pandas DataFrame)\n",
        "        \n",
        "        y_train     (pandas DataFrame)  optional\n",
        "                                        required only if clf has not already been fitted \n",
        "        \n",
        "        top_n       (int)               Plot the top_n most-important features\n",
        "                                        Default: 10\n",
        "                                        \n",
        "        figsize     ((int,int))         The physical size of the plot\n",
        "                                        Default: (8,8)\n",
        "        \n",
        "        print_table (boolean)           If True, print out the table of feature importances\n",
        "                                        Default: False\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "        the pandas dataframe with the features and their importance\n",
        "        \n",
        "    Author\n",
        "    ------\n",
        "        George Fisher\n",
        "    '''\n",
        "    \n",
        "    __name__ = \"plot_feature_importances\"\n",
        "    \n",
        "    import pandas as pd\n",
        "    import numpy  as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    from lightgbm.sklearn import LightGBMError\n",
        "    \n",
        "    try: \n",
        "        if not hasattr(clf, 'feature_importances_'):\n",
        "            clf.fit(X_train.values, y_train.values.ravel())\n",
        "\n",
        "            if not hasattr(clf, 'feature_importances_'):\n",
        "                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n",
        "                                    format(clf.__class__.__name__))\n",
        "                \n",
        "    except (LightGBMError, ValueError):\n",
        "        clf.fit(X_train.values, y_train.values.ravel())\n",
        "            \n",
        "    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n",
        "    feat_imp['feature'] = X_train.columns\n",
        "    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n",
        "    feat_imp = feat_imp.iloc[:top_n]\n",
        "    \n",
        "    feat_imp.sort_values(by='importance', inplace=True)\n",
        "    feat_imp = feat_imp.set_index('feature', drop=True)\n",
        "    feat_imp.plot.barh(title=title, figsize=figsize)\n",
        "    plt.xlabel('Feature Importance Score')\n",
        "    plt.show()\n",
        "    \n",
        "    if print_table:\n",
        "        from IPython.display import display\n",
        "        print(\"Top {} features in descending order of importance\".format(top_n))\n",
        "        display(feat_imp.sort_values(by='importance', ascending=False))\n",
        "        \n",
        "    return feat_imp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6TQUgCWzHFj"
      },
      "outputs": [],
      "source": [
        "my_clf = lgb.LGBMClassifier(num_leaves=31, reg_alpha=0.1, lambda_l1=0, lambda_l2=0, min_data_in_leaf=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PhETGlYzNt8"
      },
      "outputs": [],
      "source": [
        "plot_feature_importances(my_clf, X1_train, y1_train, top_n=X1_train.shape[1], title=clf.__class__.__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB_Kd_gQzmVz"
      },
      "outputs": [],
      "source": [
        "clf1 = print_auc(model1, X1_train, y1_train, X1_test, y1_test, \"Logistic Regression\")\n",
        "clf2 = print_auc(model2, X1_train, y1_train, X1_test, y1_test, \"SVM\")\n",
        "clf3 = print_auc(model3, X1_train, y1_train, X1_test, y1_test, \"Gradient Boosting\")\n",
        "clf4 = print_auc(model4, X1_train, y1_train, X1_test, y1_test, \"Random Forest\")\n",
        "clf5 = print_auc(model5, X1_train, y1_train, X1_test, y1_test, \"KNeighbors Classifier\")\n",
        "clf6 = print_auc(model6, X1_train, y1_train, X1_test, y1_test, \"ExtraTrees Classifier\")\n",
        "clf7 = print_auc(model7, X1_train, y1_train, X1_test, y1_test, \"Adaboost Classifier\")\n",
        "clf8 = print_auc(model8, X1_train, y1_train, X1_test, y1_test, \"DecisionTree  Classifier \")\n",
        "clf9 = print_auc(model9, X1_train, y1_train, X1_test, y1_test, \"GaussianNB\")\n",
        "clf10 = print_auc(model10, X1_train, y1_train, X1_test, y1_test, \"LGBM\")\n",
        "clf11 = print_auc(model10, X1_train, y1_train, X1_test, y1_test, \"XGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acsv-t5q0zh7"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = [16, 8]\n",
        "plt.plot(clf10[\"fpr\"], clf10[\"tpr\"], label=\"LGBM, auc=\"+str(clf10[\"auc\"]))\n",
        "plt.plot(clf1[\"fpr\"], clf1[\"tpr\"], label=\"LR, auc=\"+str(clf1[\"auc\"]))\n",
        "plt.plot(clf2[\"fpr\"], clf2[\"tpr\"], label=\"SVM, auc=\"+str(clf2[\"auc\"]))\n",
        "plt.plot(clf3[\"fpr\"], clf3[\"tpr\"], label=\"GB, auc=\"+str(clf3[\"auc\"]))\n",
        "plt.plot(clf6[\"fpr\"], clf6[\"tpr\"], label=\"ExtraTree, auc=\"+str(clf6[\"auc\"]))\n",
        "plt.plot(clf5[\"fpr\"], clf5[\"tpr\"], label=\"KNN, auc=\"+str(clf5[\"auc\"]))\n",
        "plt.plot(clf8[\"fpr\"], clf8[\"tpr\"], label=\"DT, auc=\"+str(clf8[\"auc\"]))\n",
        "plt.plot(clf9[\"fpr\"], clf9[\"tpr\"], label=\"GaussianNB, auc=\"+str(clf9[\"auc\"]))\n",
        "plt.plot(clf4[\"fpr\"], clf4[\"tpr\"], label=\"RF, auc=\"+str(clf4[\"auc\"]))\n",
        "plt.plot(clf7[\"fpr\"], clf7[\"tpr\"], label=\"AdaBoost, auc=\"+str(clf7[\"auc\"]))\n",
        "plt.plot(clf11[\"fpr\"], clf11[\"tpr\"], label=\"XGB, auc=\"+str(clf11[\"auc\"]))\n",
        "\n",
        "plt.xlabel(\"tpr\")\n",
        "plt.ylabel(\"fpr\")\n",
        "plt.legend(loc=4)\n",
        "plt.savefig('figure.png', dpi=300)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}